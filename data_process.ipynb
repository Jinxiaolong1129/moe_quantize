{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to autogptq_eval_result/deepseek-moe-16b-base/evaluation_results_all.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dir_path = 'autogptq_eval_result/deepseek-moe-16b-base'  # Directory containing the files\n",
    "\n",
    "def extract_bits(filename):\n",
    "    \"\"\"Extract the bits part from the filename.\"\"\"\n",
    "    try:\n",
    "        return filename.split(\"_w_bit_\")[1].split(\"_pile\")[0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "# Dictionary to hold all data, with bits as keys and metric dictionaries as values\n",
    "all_data = {}\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.startswith(\"eval_result_deepseek-moe-16b-base-gptq_w_bit_\") and filename.endswith(\"_pile\"):\n",
    "        bits = extract_bits(filename)\n",
    "        if bits:  # Ensure bits part was successfully extracted\n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = json.load(file)\n",
    "                    all_data[bits] = content\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON is invalid, skip this file\n",
    "                continue\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "# This approach ensures all keys across files are considered\n",
    "df = pd.DataFrame.from_dict(all_data, orient='index')\n",
    "\n",
    "# Optionally, sort the DataFrame by index (bits) if required\n",
    "# df.sort_index(inplace=True)\n",
    "\n",
    "df.index.name = 'Bits'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_path = os.path.join(dir_path, 'evaluation_results_all.csv')\n",
    "df.to_csv(csv_path)\n",
    "\n",
    "print(f\"CSV file has been saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e1b000f2f34f88b5fadb0930c1b568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(eval_bits): 44\n",
      "Bits: moe.shared_2.other.4+other_block_4\n",
      "MoE Average Bit: 3.9393939393939394\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 3.941434076362471\n",
      "=========================\n",
      "Bits: moe.shared_4.top30_8.other_2+other_block.8\n",
      "MoE Average Bit: 4.787878787878788\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 4.89600604721097\n",
      "=========================\n",
      "Bits: moe.shared_4.top15_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.515151515151515\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.6997839108036383\n",
      "=========================\n",
      "Bits: moe.shared_8.top30_4.other_2+other_block.4\n",
      "MoE Average Bit: 3.090909090909091\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 3.121511145437067\n",
      "=========================\n",
      "Bits: moe.shared_4.top2_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.121212121212121\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.1844563672366055\n",
      "=========================\n",
      "Bits: moe.shared_8.top2_8.other_2+other_block.4\n",
      "MoE Average Bit: 2.3636363636363638\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.4187200617867206\n",
      "=========================\n",
      "Bits: moe.shared_8.top5_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.3333333333333335\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.389437099967956\n",
      "=========================\n",
      "Bits: moe.shared_4.top20_8.other_2+other_block.8\n",
      "MoE Average Bit: 3.878787878787879\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 4.017517192648038\n",
      "=========================\n",
      "Bits: moe.shared_8.top2_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.242424242424242\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.301588214511663\n",
      "=========================\n",
      "Bits: moe.shared_4.other.2+other_block.8\n",
      "MoE Average Bit: 2.0606060606060606\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.2605394835221717\n",
      "=========================\n",
      "Bits: all_8\n",
      "MoE Average Bit: 8.0\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 8.0\n",
      "=========================\n",
      "Bits: moe.shared_4.top25_8.other_2+other_block.8\n",
      "MoE Average Bit: 4.333333333333333\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 4.456761619929504\n",
      "=========================\n",
      "Bits: all_2\n",
      "MoE Average Bit: 2.0\n",
      "Self-Attention Average Bit: 2.0\n",
      "Average Bit: 2.0\n",
      "=========================\n",
      "Bits: moe.shared_4.top2_8.other_2+other_block.4\n",
      "MoE Average Bit: 2.242424242424242\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.301588214511663\n",
      "=========================\n",
      "Bits: moe.shared_8.top35_4.other_2+other_block.4\n",
      "MoE Average Bit: 3.242424242424242\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 3.2679259545308894\n",
      "=========================\n",
      "Bits: all_4\n",
      "MoE Average Bit: 4.0\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 4.0\n",
      "=========================\n",
      "Bits: moe.shared_8.top1_8.other_2+other_block.8\n",
      "MoE Average Bit: 2.272727272727273\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.465520216253523\n",
      "=========================\n",
      "Bits: moe.shared_4.top25_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.8181818181818183\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.9926135289912823\n",
      "=========================\n",
      "Bits: moe.shared_4.top10_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.3636363636363638\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.553369101709816\n",
      "=========================\n",
      "Bits: moe.shared_8.top45_4.other_2+other_block.4\n",
      "MoE Average Bit: 3.5454545454545454\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 3.5607555727185334\n",
      "=========================\n",
      "Bits: moe.shared_4.top30_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.9696969696969697\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 3.1390283380851045\n",
      "=========================\n",
      "Bits: moe.shared_4.top35_4.other_2+other_block.8\n",
      "MoE Average Bit: 3.121212121212121\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 3.2854431471789267\n",
      "=========================\n",
      "Bits: moe.shared_8.top15_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.6363636363636362\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.6822667181556006\n",
      "=========================\n",
      "Bits: moe.all_mlp.4+other_block.8\n",
      "MoE Average Bit: 4.0\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 4.1346490399230955\n",
      "=========================\n",
      "Bits: moe.shared_8.top1_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.212121212121212\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.272305252692899\n",
      "=========================\n",
      "Bits: moe.shared_4.top15_8.other_2+other_block.8\n",
      "MoE Average Bit: 3.4242424242424243\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 3.578272765366571\n",
      "=========================\n",
      "Bits: moe.shared_8.top25_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.9393939393939394\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.975096336343245\n",
      "=========================\n",
      "Bits: moe.shared_4.top10_8.other_2+other_block.8\n",
      "MoE Average Bit: 2.9696969696969697\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 3.1390283380851045\n",
      "=========================\n",
      "Bits: moe.shared_4.top1_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.090909090909091\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.289822445340936\n",
      "=========================\n",
      "Bits: moe.shared_8.top1_8.other_2+other_block.4\n",
      "MoE Average Bit: 2.272727272727273\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.3308711763304273\n",
      "=========================\n",
      "Bits: moe.shared_4.top2_8.other_2+other_block.8\n",
      "MoE Average Bit: 2.242424242424242\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.4362372544347584\n",
      "=========================\n",
      "Bits: moe.shared_4.top2_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.121212121212121\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.3191054071597006\n",
      "=========================\n",
      "Bits: moe.shared_4.top40_4.other_2+other_block.8\n",
      "MoE Average Bit: 3.272727272727273\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 3.431857956272749\n",
      "=========================\n",
      "Bits: moe.shared_2.other.4+other_block.8\n",
      "MoE Average Bit: 3.9393939393939394\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 4.076083116285567\n",
      "=========================\n",
      "Bits: moe.shared_4.top1_8.other_2+other_block.8\n",
      "MoE Average Bit: 2.1515151515151514\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.348388368978465\n",
      "=========================\n",
      "Bits: moe.shared_4.top20_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.6666666666666665\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.8461987198974605\n",
      "=========================\n",
      "Bits: moe.shared_4.top5_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.212121212121212\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.406954292615994\n",
      "=========================\n",
      "Bits: moe.shared_4.top5_8.other_2+other_block.8\n",
      "MoE Average Bit: 2.515151515151515\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.6997839108036383\n",
      "=========================\n",
      "Bits: moe.shared_4.other.2+other_block_4\n",
      "MoE Average Bit: 2.0606060606060606\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.1258904435990766\n",
      "=========================\n",
      "Bits: moe.shared_8.top2_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.242424242424242\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.4362372544347584\n",
      "=========================\n",
      "Bits: moe.shared_8.top10_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.484848484848485\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.5358519090617784\n",
      "=========================\n",
      "Bits: moe.all_mlp.2+other_block.4\n",
      "MoE Average Bit: 2.0\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.0673245199615478\n",
      "=========================\n",
      "Bits: moe.shared_8.top1_4.other_2+other_block.8\n",
      "MoE Average Bit: 2.212121212121212\n",
      "Self-Attention Average Bit: 8.0\n",
      "Average Bit: 2.406954292615994\n",
      "=========================\n",
      "Bits: moe.shared_8.top20_4.other_2+other_block.4\n",
      "MoE Average Bit: 2.787878787878788\n",
      "Self-Attention Average Bit: 4.0\n",
      "Average Bit: 2.828681527249423\n",
      "=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log data has been saved to autogptq_eval_result/deepseek_bits_data.csv.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from argparse import ArgumentParser\n",
    "import csv\n",
    "import torch\n",
    "from auto_gptq import moe_quantize_config\n",
    "\n",
    "class parse_args:\n",
    "    def __init__(self, bits):\n",
    "        self.bits = bits\n",
    "model = AutoModelForCausalLM.from_pretrained('deepseek-ai/deepseek-moe-16b-base', torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "log_data = []\n",
    "\n",
    "def extract_bits(filename):\n",
    "    \"\"\"Extract the bits part from the filename.\"\"\"\n",
    "    try:\n",
    "        return filename.split(\"_w_bit_\")[1].split(\"_pile\")[0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "eval_bits = []\n",
    "\n",
    "for filename in os.listdir('autogptq_eval_result/deepseek-moe-16b-base'):\n",
    "    if filename.startswith(\"eval_result_deepseek-moe-16b-base-gptq_w_bit_\") and filename.endswith(\"_pile\"):\n",
    "        bits = extract_bits(filename)\n",
    "        eval_bits.append(bits)\n",
    "\n",
    "print(f\"len(eval_bits): {len(eval_bits)}\")\n",
    "\n",
    "\n",
    "for bits in eval_bits:\n",
    "    args = parse_args(bits)\n",
    "    \n",
    "    deeepseek_bit = moe_quantize_config(args)\n",
    "    \n",
    "    total_bits_moe = 0\n",
    "    total_params_moe = 0\n",
    "    total_bits_self_attn = 0\n",
    "    total_params_self_attn = 0\n",
    "    total_bits = 0\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            weight = module.weight.data\n",
    "            num_params = weight.numel()  # Total number of parameters in the module\n",
    "            \n",
    "            if name in deeepseek_bit:\n",
    "                bit = deeepseek_bit[name]\n",
    "                total_bits += num_params * bit  # Accumulate total bits for all specified modules\n",
    "                total_params += num_params\n",
    "                \n",
    "            if ('experts' in name or 'shared_experts' in name) and name in deeepseek_bit:\n",
    "                bit = deeepseek_bit[name]\n",
    "                total_bits_moe += num_params * bit\n",
    "                total_params_moe += num_params\n",
    "                \n",
    "                # print(f'name {name} | bit {bit}')\n",
    "                # print(f'total_bits_moe {total_bits_moe} | num_params {num_params} | bit {bit}')\n",
    "            elif 'self_attn' in name and name in deeepseek_bit:\n",
    "                bit = deeepseek_bit[name]\n",
    "                total_bits_self_attn += num_params * bit\n",
    "                total_params_self_attn += num_params\n",
    "        \n",
    "    \n",
    "    average_bit_moe = total_bits_moe / total_params_moe if total_params_moe > 0 else 0\n",
    "    average_bit_self_attn = total_bits_self_attn / total_params_self_attn if total_params_self_attn > 0 else 0\n",
    "    average_bit = total_bits / total_params if total_params > 0 else 0\n",
    "    print(f\"Bits: {bits}\")\n",
    "    print(f\"MoE Average Bit: {average_bit_moe}\")\n",
    "    print(f\"Self-Attention Average Bit: {average_bit_self_attn}\")\n",
    "    print(f\"Average Bit: {average_bit}\")\n",
    "    print('=========================')\n",
    "    \n",
    "    data = {\n",
    "        \"Bits\": bits,\n",
    "        \"MoE Average Bit\": average_bit_moe,\n",
    "        \"Self-Attention Average Bit\": average_bit_self_attn,\n",
    "        \"Average Bit\": average_bit\n",
    "    }\n",
    "    \n",
    "    # Add the data to the list\n",
    "    log_data.append(data)\n",
    "\n",
    "fieldnames = [\"Bits\", \"MoE Average Bit\", \"Self-Attention Average Bit\", \"Average Bit\"]\n",
    "\n",
    "# Open a CSV file to write the data\n",
    "save_path = 'autogptq_eval_result/deepseek_bits_data.csv'\n",
    "with open(save_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the log data\n",
    "    writer.writerows(log_data)\n",
    "\n",
    "print(f\"Log data has been saved to {save_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('/home/LeiFeng/xiaolong/moe_quantize/autogptq_eval_result/deepseek-moe-16b-base/deepseek_bits_data.csv')\n",
    "df2 = pd.read_csv('/home/LeiFeng/xiaolong/moe_quantize/autogptq_eval_result/deepseek-moe-16b-base/evaluation_results_all.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Bits', how='outer')\n",
    "\n",
    "merged_df.to_csv('/home/LeiFeng/xiaolong/moe_quantize/autogptq_eval_result/deepseek-moe-16b-base/evaluation_results_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "qweight = torch.tensor(\n",
    "[[-4],[-6]]\n",
    ", dtype=torch.int32)  \n",
    "\n",
    "\n",
    "bits = 2\n",
    "shifts = torch.arange(0, 32, bits)\n",
    "iweights = torch.bitwise_right_shift(qweight[:, :, None], shifts[None, None, :]).to(\n",
    "    torch.int8  # smallest dtype available\n",
    ")\n",
    "iweights = iweights.view(iweights.shape[0], -1)\n",
    "iweights = torch.bitwise_and(iweights, (2**bits) - 1)\n",
    "    \n",
    "print(iweights)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16]),\n",
       " tensor(254, dtype=torch.int32),\n",
       " tensor(72, dtype=torch.int32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qweight:  tensor([[-4],\n",
      "        [-6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# intweight = torch.randint(low=0, high=256, size=shape, dtype=torch.int32)\n",
    "# (intweight.shape[0], intweight.shape[1] // 32 * w_bit),\n",
    "\n",
    "# intweight = torch.tensor([[168,  77,  94,  72],\n",
    "#         [234, 254, 191, 123]], dtype=torch.int32)\n",
    "\n",
    "import torch\n",
    "\n",
    "shape = (2, 4)\n",
    "w_bit = 2\n",
    "\n",
    "intweight = torch.tensor([[168,  77,  94,  72] * (8//w_bit),\n",
    "        [234, 254, 191, 123]*(8//w_bit)], dtype=torch.int32)\n",
    "\n",
    "\n",
    "intweight.shape, intweight.max(), intweight.min()\n",
    "\n",
    "\n",
    "qweight = torch.zeros(\n",
    "    (intweight.shape[0], intweight.shape[1] * w_bit// 32 ),\n",
    "    dtype=torch.int32,\n",
    "    device=intweight.device,\n",
    ")\n",
    "qweight.shape\n",
    "pack_num = 32 // w_bit\n",
    "\n",
    "for col in range(intweight.shape[1] // pack_num):\n",
    "    if w_bit == 4:\n",
    "        order_map = [0, 2, 4, 6, 1, 3, 5, 7]\n",
    "    elif w_bit == 8:\n",
    "        order_map = list(range(4))  # Order map for 8-bit quantization\n",
    "        print(f'order_map: {order_map}')\n",
    "    elif w_bit == 2:\n",
    "        order_map = list(range(pack_num))  # Order map for 2-bit quantization\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 2-bit and 4-bit quantizations are supported.\")\n",
    "\n",
    "    for i in range(pack_num):\n",
    "        idx = col * pack_num + order_map[i]\n",
    "        if idx < intweight.shape[1]:  # Check to avoid 'index out of range'\n",
    "            qweight_col = intweight[:, idx]\n",
    "            qweight[:, col] |= qweight_col << (i * w_bit)\n",
    "print('qweight: ', qweight)       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qweight:  tensor([[-4],\n",
      "        [-6]], dtype=torch.int32)\n",
      "intweight:  tensor([[168,  77,  94,  72, 168,  77,  94,  72, 168,  77,  94,  72, 168,  77,\n",
      "          94,  72],\n",
      "        [234, 254, 191, 123, 234, 254, 191, 123, 234, 254, 191, 123, 234, 254,\n",
      "         191, 123]], dtype=torch.int32)\n",
      "iweights:  tensor([[252, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "         255, 255],\n",
      "        [250, 254, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "         255, 255]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w_bit = 2\n",
    "\n",
    "intweight = torch.tensor([[168,  77,  94,  72] * (8//w_bit),\n",
    "        [234, 254, 191, 123]*(8//w_bit)], dtype=torch.int32)\n",
    "\n",
    "\n",
    "\n",
    "intweight.shape, intweight.max(), intweight.min()\n",
    "\n",
    "\n",
    "qweight = torch.zeros(\n",
    "    (intweight.shape[0], intweight.shape[1] * w_bit// 32 ),\n",
    "    dtype=torch.int32,\n",
    "    device=intweight.device,\n",
    ")\n",
    "qweight.shape\n",
    "pack_num = 32 // w_bit\n",
    "\n",
    "for col in range(intweight.shape[1] // pack_num):\n",
    "    if w_bit == 4:\n",
    "        order_map = [0, 2, 4, 6, 1, 3, 5, 7]\n",
    "    elif w_bit == 8:\n",
    "        order_map = list(range(4))  # Order map for 8-bit quantization\n",
    "        print(f'order_map: {order_map}')\n",
    "    elif w_bit == 2:\n",
    "        order_map = list(range(pack_num))  # Order map for 2-bit quantization\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 2-bit and 4-bit quantizations are supported.\")\n",
    "\n",
    "    for i in range(pack_num):\n",
    "        idx = col * pack_num + order_map[i]\n",
    "        if idx < intweight.shape[1]:  # Check to avoid 'index out of range'\n",
    "            qweight_col = intweight[:, idx]\n",
    "            qweight[:, col] |= qweight_col << (i * w_bit)\n",
    "print('qweight: ', qweight)    \n",
    "print('intweight: ', intweight)\n",
    "\n",
    "shifts = torch.arange(0, 32, w_bit)\n",
    "\n",
    "iweights = torch.bitwise_right_shift(qweight[:, :, None], shifts[None, None, :]).to(\n",
    "    torch.uint8  \n",
    ")\n",
    "iweights = iweights.view(iweights.shape[0], -1)\n",
    "print('iweights: ', iweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = torch.arange(0, 32, bits, device=qweight.device)\n",
    "masks = torch.tensor([0xFF << shift for shift in shifts], device=qweight.device)\n",
    "\n",
    "# Unpack weights\n",
    "iweights = torch.bitwise_and(qweight[:, :, None], masks[None, None, :])\n",
    "iweights = torch.bitwise_right_shift(iweights, shifts[None, None, :])\n",
    "iweights = iweights.to(torch.int8)\n",
    "iweights = iweights.view(iweights.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data type to avoid overflow\n",
    "qweight = torch.tensor([\n",
    "[[1214139816],\n",
    "        [2076180202]]\n",
    "], dtype=torch.int64)  # Using 64-bit integer to prevent overflow\n",
    "\n",
    "from bitstring import Bits\n",
    "\n",
    "def to_2bit_binary(num):\n",
    "    # Ensure the number is within the valid range for 2-bit representation\n",
    "    if num < 0 or num > 3:\n",
    "        raise ValueError(\"Number must be between 0 and 3 for 2-bit representation\")\n",
    "    \n",
    "    # Convert the number to a 2-bit binary string\n",
    "    binary_str = Bits(uint=num, length=2).bin\n",
    "    \n",
    "    return binary_str\n",
    "\n",
    "\n",
    "# Convert each element in the tensor to binary strings\n",
    "qweight_binary = [[bin(val.item()) for val in row] for row in qweight]\n",
    "\n",
    "qweight_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-88,  77,  94,  72],\n",
       "         [-22,  -2, -65, 123]], dtype=torch.int8),\n",
       " tensor([[168,  77,  94,  72],\n",
       "         [234, 254, 191, 123]], dtype=torch.int32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original intweight tensor\n",
    "intweight = torch.tensor([[168,  77,  94,  72],\n",
    "                          [234, 254, 191, 123]], dtype=torch.int32)\n",
    "\n",
    "# Packed qweight tensor\n",
    "qweight = torch.tensor([[1214139816],\n",
    "                        [2076180202]], dtype=torch.int32)\n",
    "\n",
    "# Unpacking logic correction\n",
    "w_bit = 8\n",
    "shifts = torch.arange(0, 32, w_bit)\n",
    "mask = (1 << w_bit) - 1  # Create a mask to isolate w_bit bits\n",
    "\n",
    "# Unpack qweight\n",
    "iweights_corrected = torch.bitwise_and(torch.bitwise_right_shift(qweight[:, :, None], shifts[None, None, :]), mask).to(torch.int8)\n",
    "\n",
    "# Interpret the values as signed int8\n",
    "iweights_corrected = iweights_corrected.view(iweights_corrected.shape[0], -1)\n",
    "\n",
    "iweights_corrected, intweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qweight:  tensor([[ 960051513],\n",
      "        [-454761244]], dtype=torch.int32)\n",
      "intweight:  tensor([[1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0],\n",
      "        [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]], dtype=torch.int32)\n",
      "intweight_unpacked:  tensor([[ 1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0],\n",
      "        [ 0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2,  3,  0,  1,  2, -1]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w_bit = 2\n",
    "\n",
    "intweight = torch.tensor([[1,2,3,0] * (8//w_bit),\n",
    "        [0,1,2,3]*(8//w_bit)], dtype=torch.int32)\n",
    "\n",
    "intweight.shape, intweight.max(), intweight.min()\n",
    "\n",
    "\n",
    "qweight = torch.zeros(\n",
    "    (intweight.shape[0], intweight.shape[1] * w_bit// 32 ),\n",
    "    dtype=torch.int32,\n",
    "    device=intweight.device,\n",
    ")\n",
    "qweight.shape\n",
    "pack_num = 32 // w_bit\n",
    "\n",
    "for col in range(intweight.shape[1] // pack_num):\n",
    "    if w_bit == 4:\n",
    "        order_map = [0, 2, 4, 6, 1, 3, 5, 7]\n",
    "    elif w_bit == 8:\n",
    "        order_map = list(range(4))  # Order map for 8-bit quantization\n",
    "        print(f'order_map: {order_map}')\n",
    "    elif w_bit == 2:\n",
    "        order_map = list(range(pack_num))  # Order map for 2-bit quantization\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 2-bit and 4-bit quantizations are supported.\")\n",
    "\n",
    "    for i in range(pack_num):\n",
    "        idx = col * pack_num + order_map[i]\n",
    "        if idx < intweight.shape[1]:  # Check to avoid 'index out of range'\n",
    "            qweight_col = intweight[:, idx]\n",
    "            qweight[:, col] |= qweight_col << (i * w_bit)\n",
    "print('qweight: ', qweight)    \n",
    "print('intweight: ', intweight)\n",
    "\n",
    "\n",
    "qweight_shape = qweight.shape  \n",
    "\n",
    "intweight_unpacked = torch.zeros((qweight_shape[0], qweight_shape[1] * pack_num), dtype=torch.int32)\n",
    "\n",
    "for col in range(qweight_shape[1]):\n",
    "    for i in range(pack_num):\n",
    "        # Calculate the mask to isolate the bits for the current value\n",
    "        mask = (1 << w_bit) - 1\n",
    "        mask = mask << (i * w_bit)\n",
    "        \n",
    "        # Extract the bits for the current value\n",
    "        extracted_bits = (qweight[:, col] & mask) >> (i * w_bit)\n",
    "        \n",
    "        # Place the extracted bits in the correct position in intweight_unpacked\n",
    "        intweight_unpacked[:, col * pack_num + i] = extracted_bits\n",
    "\n",
    "print('intweight_unpacked: ', intweight_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1]],\n",
      "\n",
      "        [[3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0]]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def split_int32_to_int2(num):\n",
    "    int2_nums = torch.zeros(16, dtype=torch.int8)\n",
    "    for i in range(16):\n",
    "        int2_num = (num >> (30 - i * 2)) & 0b11\n",
    "        int2_nums[i] = int2_num\n",
    "    return int2_nums\n",
    "\n",
    "def split_tensor_to_int2(tensor):\n",
    "    # Preallocate the tensor to hold the results\n",
    "    # tensor.shape[0] is the number of elements, 16 is the new size for each element\n",
    "    results_shape = tensor.shape + (16,)  # Adjust according to your specific shape\n",
    "    int2_tensor = torch.zeros(results_shape, dtype=torch.int8)\n",
    "    \n",
    "    # Iterate through each element to apply the function\n",
    "    for i in range(tensor.size(0)):\n",
    "        for j in range(tensor.size(1)):  # Assuming 2D tensor for simplicity; adjust as needed\n",
    "            int2_tensor[i, j] = split_int32_to_int2(tensor[i, j])\n",
    "    \n",
    "    return int2_tensor\n",
    "\n",
    "# Define the tensor\n",
    "qweight = torch.tensor([[960051513], [-454761244]], dtype=torch.int32)\n",
    "\n",
    "# Apply the function to the tensor\n",
    "int2_qweight = split_tensor_to_int2(qweight)\n",
    "\n",
    "print(int2_qweight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original int32 tensor: tensor([[1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0],\n",
      "        [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]], dtype=torch.int32)\n",
      "Original int32 tensor: torch.Size([2, 16])\n",
      "Converted to 16 int2 values: tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]]], dtype=torch.int32)\n",
      "Shape of converted tensor: torch.Size([2, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def split_int32_to_int2(tensor):\n",
    "    # Create a tensor of shift values\n",
    "    shifts = torch.arange(30, -2, -2, dtype=torch.int32).reshape(1, -1)\n",
    "    # Create the mask by shifting 0b11 to the right positions\n",
    "    mask = 3 << shifts  # Broadcasted shift operation\n",
    "    \n",
    "    # Apply the mask and shift the results to the right\n",
    "    int2_tensor = (tensor.unsqueeze(-1) & mask) >> shifts\n",
    "    \n",
    "    return int2_tensor\n",
    "\n",
    "# Example usage\n",
    "intweight = torch.tensor([[1, 2, 3, 0] * (8//2),\n",
    "                          [0, 1, 2, 3] * (8//2)], dtype=torch.int32)\n",
    "print\n",
    "# Convert to 16 int2 numbers\n",
    "int2_intweight = split_int32_to_int2(intweight)\n",
    "\n",
    "# int2_intweight will have a shape of (original_shape, 16) representing the 2-bit values\n",
    "print('Original int32 tensor:', intweight)\n",
    "print('Original int32 tensor:', intweight.shape)\n",
    "print('Converted to 16 int2 values:', int2_intweight)\n",
    "print('Shape of converted tensor:', int2_intweight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_25', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_25', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_10', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_20', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_15', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_27', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_5', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_10', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_27', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+endlayer_15', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_20', 'deepseek-moe-16b-base-gptq_w_bit_moe.shared_8.top25_4.other_2+other_block.4+startlayer_5'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_directories(path):\n",
    "    try:\n",
    "        # 使用 os.listdir 获取目录内容\n",
    "        directory_contents = os.listdir(path)\n",
    "        # 使用 os.path.isdir 判断哪些是目录\n",
    "        directories = {item for item in directory_contents if os.path.isdir(os.path.join(path, item))}\n",
    "        return set(directories)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return set()\n",
    "\n",
    "# 替换此路径为你的目标目录路径\n",
    "path = '/home/LeiFeng/xiaolong/moe_quantize/autogptq_deepseek-ai'\n",
    "directories = list_directories(path)\n",
    "print(directories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoawq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
